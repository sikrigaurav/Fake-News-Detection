{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "kaggle2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_FMihlIVWmD"
      },
      "source": [
        "## Importing required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEVKyay7ma70"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional, Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from pandas import DataFrame\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('wordnet')\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS5OiImmVd0x"
      },
      "source": [
        "## Loading training and testing dataset along with the sample submission dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkFaeHu9mgsB"
      },
      "source": [
        "test = pd.read_csv('test.csv')\n",
        "train = pd.read_csv('train.csv')\n",
        "sample = pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09uob35XVk4r"
      },
      "source": [
        "## Combining the features of the training and test dataset (titile, text and date)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "839fnrx_HoH-"
      },
      "source": [
        "X_train = train['title'] + \" \" + train['text'] + \" \" + train['date']\n",
        "X_test = test['title'] + \" \" + test['text'] + \" \" + test['date']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D8ZQPBqV3Gv"
      },
      "source": [
        "## Initializing a few variables and creating padded sequence for training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QyVxmUxm4xi"
      },
      "source": [
        "max_words = 3000\n",
        "max_len = 512\n",
        "embed_dim = 100\n",
        "lstm_out = 256\n",
        "batch_size = 64\n",
        "token = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
        "token.fit_on_texts(X_train.values)\n",
        "sequences = token.texts_to_sequences(X_train.values)\n",
        "train_sequences_padded = pad_sequences(sequences, maxlen=max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhqTp0DWWoxv"
      },
      "source": [
        "## Creating a model using and embedded layer and Bidirectional LSTM followed by a couple of dense layers. I have used dropout and batch normalization between the dense layers. The optimizer used is Adamax and the loss function used is binary_crossentropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muLNCYb0kqPi"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embed_dim, input_length = max_len))\n",
        "model.add(Bidirectional(LSTM(lstm_out)))\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('selu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('selu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, name='out_layer'))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adamax', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdTfnJx5XCio"
      },
      "source": [
        "## Printing the summary of the mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMBb-vq2XB7j"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKSVsqQlXGnQ"
      },
      "source": [
        "## Initializing early stopping and model checkpoint callback functions which will be used while training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHozVJrmco_Z"
      },
      "source": [
        "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=200)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh-KDRICXR7j"
      },
      "source": [
        "## Training the model for 10 epochs and using 30% of the training data as validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv_L0HMinUjR"
      },
      "source": [
        "model.fit(train_sequences_padded, train['is_fake'], batch_size=batch_size, epochs = 10, validation_split=0.3, callbacks=[mc, es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZmXGA9bXj-F"
      },
      "source": [
        "## Converting test data into padded sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy1WJ-zVvanJ"
      },
      "source": [
        "test_sequences = token.texts_to_sequences(X_test)\n",
        "test_sequences_padded = pad_sequences(test_sequences,\n",
        "                                       maxlen=max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZbljCNXXn6U"
      },
      "source": [
        "## Predicting results for test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8BKb6z5w1Ri"
      },
      "source": [
        "res = model.predict(test_sequences_padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMUDnwKgXrd9"
      },
      "source": [
        "## Converting the probabilities into labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2xqQ6CUsXan"
      },
      "source": [
        "res = (model.predict(test_sequences_padded) > 0.5).astype(\"int\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xB7yEoEX1xa"
      },
      "source": [
        "## Creating a dataframe of test ids and predicted labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykpEbEu2JQUN"
      },
      "source": [
        "pred_list = DataFrame(res, columns = ['is_fake'])\n",
        "sample_ids = DataFrame(sample['id'], columns=['id'])\n",
        "result = pd.concat([sample_ids, pred_list], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvJa-biIYe9T"
      },
      "source": [
        "## Saving the results as a csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju5PJo4lqmTg"
      },
      "source": [
        "result.to_csv('res2.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}